{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testmymodel(model, features, labels):\n",
    "    try:\n",
    "        X = np.load(features)\n",
    "    except FileNotFoundError:\n",
    "        raise(\"didnt find the features file\")\n",
    "\n",
    "    try:\n",
    "        Y = np.load(labels)\n",
    "    except FileNotFoundError:\n",
    "        raise(\"didnt find the labels file\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "    model.fit(X=X_train, y=y_train)\n",
    "    prediction_accuracy = model.score(X_test, y_test)\n",
    "    return prediction_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X = np.load(\"cifar10_features.npy\")\n",
    "except FileNotFoundError:\n",
    "    raise(\"didnt find the features file\")\n",
    "\n",
    "try:\n",
    "    Y = np.load(\"cifar10_labels.npy\")\n",
    "except FileNotFoundError:\n",
    "    raise(\"didnt find the labels file\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of the softmax = 0.9629333333333333\n"
     ]
    }
   ],
   "source": [
    "softMax = LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "prediction_accuracy = testmymodel(softMax, \"cifar10_features.npy\", \"cifar10_labels.npy\")\n",
    "print(f\"the accuracy of the softmax = {prediction_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.81 s ± 147 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "softMax.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of the one vs all = 0.961\n"
     ]
    }
   ],
   "source": [
    "ova = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
    "prediction_accuracy2 = testmymodel(ova, \"cifar10_features.npy\", \"cifar10_labels.npy\")\n",
    "print(f\"the accuracy of the one vs all = {prediction_accuracy2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "877 ms ± 76.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "ova.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we see above the two algorithms have the same accuracy but there is a huge gap in the run time (the one vs all  the run time of the softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft max loss = 0.10768597462451053\n",
      "one versus all loss = 0.13459723836922313\n"
     ]
    }
   ],
   "source": [
    "softMax_loss = log_loss(y_true=y_test, y_pred=softMax.predict_proba(X_test))\n",
    "print(f\"soft max loss = {softMax_loss}\")\n",
    "ova_loss = log_loss(y_true=y_test, y_pred=ova.predict_proba(X_test))\n",
    "print(f\"one versus all loss = {ova_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft max f1-mean = 0.9631573951463324\n",
      "one versus all f1-mean = 0.961210892245097\n"
     ]
    }
   ],
   "source": [
    "softMax_f1_mean = f1_score(y_true=y_test, y_pred=softMax.predict(X_test), average='macro')\n",
    "print(f\"soft max f1-mean = {softMax_f1_mean}\")\n",
    "ova_f1_mean = f1_score(y_true=y_test, y_pred=ova.predict(X_test), average='macro')\n",
    "print(f\"one versus all f1-mean = {ova_f1_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1402    2   13    5    5    3    2   10   17    5]\n",
      " [   2 1464    4    1    0    1    0    1    5    7]\n",
      " [  11    1 1370   15   16   15    5    3    1    3]\n",
      " [  14    5   15 1450   12   51    5   10    3    4]\n",
      " [   6    0   13   15 1466    5    4    9    0    1]\n",
      " [   2    3   13   45   12 1437    8   11    2    1]\n",
      " [   5    5    9   17    4    6 1415    0    1    1]\n",
      " [   3    0    4   17    9    7    0 1456    0    1]\n",
      " [  13    2    1    5    1    0    2    0 1480    6]\n",
      " [  12    7    5    6    0    4    2    3    5 1475]]\n"
     ]
    }
   ],
   "source": [
    "ova_confusion_matrix = confusion_matrix(y_true=y_test, y_pred=ova.predict(X_test))\n",
    "print(ova_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the two classes that the model finds it the most hard to tell the differences between them are the classes with the values 3 and 5 (cats and dogs), it predicts 51 cats as dogs and 45 dogs as cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one versus all new 0.9633333333333334\n",
      "[[1446   57]\n",
      " [  53 1444]]\n"
     ]
    }
   ],
   "source": [
    "Xnew = []\n",
    "Ynew = []\n",
    "for i in range(len(Y)):\n",
    "    if Y[i] == 3 or Y[i] == 5 :\n",
    "        Xnew.append(X[i])\n",
    "        Ynew.append(Y[i])\n",
    "\n",
    "Xnew_train, Xnew_test, ynew_train, ynew_test = train_test_split(Xnew, Ynew, test_size=0.3, random_state=42)\n",
    "ovaNew = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
    "ovaNew.fit(X=Xnew_train, y=ynew_train)\n",
    "\n",
    "prediction_accuracy_new = ovaNew.score(Xnew_test, ynew_test)\n",
    "print(f\"one versus all new {prediction_accuracy_new}\")\n",
    "\n",
    "ova_confusion_matrix_new = confusion_matrix(y_true=ynew_test, y_pred=ovaNew.predict(Xnew_test))\n",
    "print(ova_confusion_matrix_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the accuracy is the same and the confusion matrix is the same too (we think the problem is not because of the model its because of the features we have here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

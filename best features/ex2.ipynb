{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# digits: 10; # samples: 1797; # features 64\n",
      "[0 1 2 ... 8 9 8]\n",
      "[[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "data, labels = load_digits(return_X_y=True)\n",
    "(n_samples, n_features), n_digits = data.shape, np.unique(labels).size\n",
    "\n",
    "print(f\"# digits: {n_digits}; # samples: {n_samples}; # features {n_features}\")\n",
    "print(labels)\n",
    "print(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_C_accuracy(features):\n",
    "    X_train_New = X_train[:, features]\n",
    "    X_test_New = X_test[:, features]\n",
    "    best_accuracy = 0\n",
    "    best_C = 0\n",
    "    Cs = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "    for c in Cs:\n",
    "        ova = LogisticRegression(multi_class='ovr', max_iter=1000, C=c, penalty='l2')\n",
    "        ova.fit(X=X_train_New, y=y_train)\n",
    "        prediction_accuracy = ova.score(X_test_New, y_test)\n",
    "        if prediction_accuracy > best_accuracy:\n",
    "            best_accuracy = prediction_accuracy\n",
    "            best_C = c\n",
    "\n",
    "    return (best_accuracy, best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(features):\n",
    "    X_train_New = X_train[:, features]\n",
    "    X_test_New = X_test[:, features]\n",
    "    best_accuracy = 0\n",
    "    ova = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
    "    ova.fit(X=X_train_New, y=y_train)\n",
    "    prediction_accuracy = ova.score(X_test_New, y_test)\n",
    "    if prediction_accuracy > best_accuracy:\n",
    "        best_accuracy = prediction_accuracy\n",
    "    \n",
    "    return best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy():\n",
    "    selected_features = []\n",
    "    remaining_features = list(range(data.shape[1]))\n",
    "    for _ in range(5):\n",
    "        best_accuracy = 0\n",
    "        best_feature  = -1\n",
    "\n",
    "        for feature in remaining_features:\n",
    "            accuracy = trainModel(selected_features+[feature])\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_feature = feature\n",
    "                best_accuracy = accuracy\n",
    "\n",
    "        selected_features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)\n",
    "\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuers is [36, 33, 42, 21, 26]\n",
      "best accuracy is 0.7555555555555555\n",
      "best C is 1\n"
     ]
    }
   ],
   "source": [
    "greedy_features = greedy()\n",
    "greedy_accuracy, greedy_C = best_C_accuracy(greedy_features)\n",
    "print(f\"featuers is {greedy_features}\")\n",
    "print(f\"best accuracy is {greedy_accuracy}\")\n",
    "print(f\"best C is {greedy_C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information():\n",
    "    dataNew = data.copy()\n",
    "    for row in range(dataNew.shape[0]):\n",
    "        for col in range(dataNew.shape[1]): \n",
    "            if dataNew[row][col] >= 0 and dataNew[row][col] <= 4:\n",
    "                dataNew[row][col] = 0\n",
    "            elif dataNew[row][col] >= 5 and dataNew[row][col] <= 10:\n",
    "                dataNew[row][col] = 1\n",
    "            else:\n",
    "                dataNew[row][col] = 2\n",
    "\n",
    "    mi = mutual_info_classif(dataNew, labels)\n",
    "    sorted_arguments = mi.argsort()\n",
    "    return sorted_arguments[-5:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuers is [21 28 43 26 42]\n",
      "best accuracy is 0.6805555555555556\n",
      "best C is 0.1\n"
     ]
    }
   ],
   "source": [
    "MI_features = mutual_information()\n",
    "MI_accuracy, MI_C = best_C_accuracy(MI_features)\n",
    "print(f\"featuers is {MI_features}\")\n",
    "print(f\"best accuracy is {MI_accuracy}\")\n",
    "print(f\"best C is {MI_C}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - the greedy has more run time as we always knew "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - they have the same best C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - the accuracy of greedy is better (0.7555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - the mi features doesnt stay the same if we run the alghorithm another time so with this the accuracy changes everytime and its field is between (0.6 and 0.725)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - so the greedy has better accuracy with worse run time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
